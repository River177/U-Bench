# Lightweight Hierarchical UTANet

> **è½»é‡åŒ–å±‚æ¬¡åŒ–åŒ»å­¦å›¾åƒåˆ†å‰²ç½‘ç»œ**
>
> é€šè¿‡å±‚æ¬¡åŒ–MoEã€ASPPå¤šå°ºåº¦æ„Ÿå—é‡ã€æ·±åº¦å¯åˆ†ç¦»å·ç§¯å®ç°è½»é‡åŒ–ï¼ŒåŒæ—¶ä¿æŒç”šè‡³æå‡æ€§èƒ½ã€‚

## ğŸ“‹ ç›®å½•

- [æ ¸å¿ƒæ€è·¯](#æ ¸å¿ƒæ€è·¯)
- [ç½‘ç»œæ¶æ„](#ç½‘ç»œæ¶æ„)
- [å…³é”®æ¨¡å—](#å…³é”®æ¨¡å—)
- [ä½¿ç”¨æ–¹æ³•](#ä½¿ç”¨æ–¹æ³•)
- [æ€§èƒ½å¯¹æ¯”](#æ€§èƒ½å¯¹æ¯”)
- [è®­ç»ƒç­–ç•¥](#è®­ç»ƒç­–ç•¥)
- [å®éªŒç»“æœ](#å®éªŒç»“æœ)
- [å‚è€ƒæ–‡çŒ®](#å‚è€ƒæ–‡çŒ®)

## ğŸ¯ æ ¸å¿ƒæ€è·¯

æœ¬æ¨¡å‹æ˜¯UTANetçš„è½»é‡åŒ–æ”¹è¿›ç‰ˆæœ¬ï¼Œä¸»è¦é€šè¿‡ä»¥ä¸‹æŠ€æœ¯å®ç°å‚æ•°é‡å’Œè®¡ç®—é‡çš„æ˜¾è‘—é™ä½ï¼š

### 1ï¸âƒ£ Hierarchical MoEï¼ˆå±‚æ¬¡åŒ–ä¸“å®¶æ··åˆï¼‰

- **4ç§æ„Ÿå—é‡ä¸“å®¶**ï¼š
  - `small`: æ ‡å‡†3Ã—3å·ç§¯ï¼Œå…³æ³¨ç»†èŠ‚ç‰¹å¾
  - `medium`: ç©ºæ´ç‡=2ï¼Œå¹³è¡¡å±€éƒ¨å’Œå…¨å±€
  - `large`: ç©ºæ´ç‡=4ï¼Œå…³æ³¨ä¸Šä¸‹æ–‡ä¿¡æ¯
  - `global`: å…¨å±€æ± åŒ–ï¼Œå»ºæ¨¡å…¨å±€ä¾èµ–

- **è½»é‡åŒ–é—¨æ§**ï¼šä½¿ç”¨å…¨å±€å¹³å‡æ± åŒ– + 1Ã—1å·ç§¯
- **Top-kè·¯ç”±**ï¼šæ¯æ¬¡åªæ¿€æ´»2ä¸ªä¸“å®¶ï¼Œå‡å°‘è®¡ç®—é‡
- **è´Ÿè½½å‡è¡¡**ï¼šé€šè¿‡å˜å¼‚ç³»æ•°æŸå¤±ç¡®ä¿ä¸“å®¶å‡åŒ€ä½¿ç”¨

### 2ï¸âƒ£ Lightweight ASPPï¼ˆè½»é‡ç©ºæ´ç©ºé—´é‡‘å­—å¡”æ± åŒ–ï¼‰

- **æ·±åº¦å¯åˆ†ç¦»å·ç§¯**ï¼šå°†æ ‡å‡†å·ç§¯åˆ†è§£ä¸ºæ·±åº¦å·ç§¯+é€ç‚¹å·ç§¯
- **å¤šå°ºåº¦èåˆ**ï¼š4ä¸ªåˆ†æ”¯ï¼ˆ1Ã—1ã€dilation=6ã€dilation=12ã€å…¨å±€æ± åŒ–ï¼‰
- **å‚æ•°å‡å°‘**ï¼šç›¸æ¯”æ ‡å‡†ASPPå‡å°‘çº¦70%å‚æ•°é‡

### 3ï¸âƒ£ Depthwise Separable Decoderï¼ˆæ·±åº¦å¯åˆ†ç¦»è§£ç å™¨ï¼‰

- **è½¬ç½®å·ç§¯ä¸Šé‡‡æ ·**ï¼šæ¢å¤ç©ºé—´åˆ†è¾¨ç‡
- **æ·±åº¦å¯åˆ†ç¦»èåˆ**ï¼šè½»é‡åŒ–çš„ç‰¹å¾èåˆ
- **è·³è·ƒè¿æ¥**ï¼šä¿ç•™å¤šå°ºåº¦ä¿¡æ¯

### 4ï¸âƒ£ çµæ´»çš„ç¼–ç å™¨é€‰æ‹©

- **ResNet34**ï¼ˆæ ‡å‡†ï¼‰ï¼šå¹³è¡¡æ€§èƒ½å’Œæ•ˆç‡
- **MobileNetV2**ï¼ˆå¯é€‰ï¼‰ï¼šè¿›ä¸€æ­¥è½»é‡åŒ–

## ğŸ—ï¸ ç½‘ç»œæ¶æ„

```
è¾“å…¥å›¾åƒ (B, 3, 224, 224)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ç¼–ç å™¨ (ResNet34 / MobileNetV2)       â”‚
â”‚  e1: 224Ã—224 â†’ e5: 14Ã—14               â”‚
â”‚  é€šé“æ•°: [64, 64, 128, 256, 512]       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ç“¶é¢ˆå±‚: Lightweight ASPP              â”‚
â”‚  å¤šå°ºåº¦ç‰¹å¾å¢å¼º (4ä¸ªåˆ†æ”¯)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ç‰¹å¾èåˆä¸è·¯ç”± (pretrained=Trueæ—¶)    â”‚
â”‚  1. å¤šå°ºåº¦èåˆ â†’ 64é€šé“                â”‚
â”‚  2. Hierarchical MoE (4ä¸“å®¶, top-2)    â”‚
â”‚  3. Dockeråˆ†å‘ â†’ 4ä¸ªå°ºåº¦               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  è§£ç å™¨: Lightweight UpBlock           â”‚
â”‚  d4: 28Ã—28 â†’ d1: 224Ã—224               â”‚
â”‚  æ·±åº¦å¯åˆ†ç¦»å·ç§¯ + è·³è·ƒè¿æ¥             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
è¾“å‡ºåˆ†å‰²å›¾ (B, n_classes, 224, 224)
```

## ğŸ”§ å…³é”®æ¨¡å—

### HierarchicalExpert

```python
class HierarchicalExpert(nn.Module):
    """ä¸åŒæ„Ÿå—é‡çš„ä¸“å®¶ç½‘ç»œ"""
    def __init__(self, emb_size: int, scale: str):
        # scale in ['small', 'medium', 'large', 'global']
        # ä½¿ç”¨æ·±åº¦å¯åˆ†ç¦»å·ç§¯ + ç©ºæ´å·ç§¯
```

**å‚æ•°é‡å¯¹æ¯”**ï¼ˆä»¥64é€šé“ä¸ºä¾‹ï¼‰ï¼š
- æ ‡å‡†3Ã—3å·ç§¯: 64 Ã— 64 Ã— 3 Ã— 3 = **36,864**
- æ·±åº¦å¯åˆ†ç¦»: 64 Ã— 3 Ã— 3 + 64 Ã— 128 = **8,768** âœ… (å‡å°‘76%)

### HierarchicalMoE

```python
class HierarchicalMoE(nn.Module):
    """å±‚æ¬¡åŒ–ä¸“å®¶æ··åˆ"""
    def forward(self, x):
        # 1. è®¡ç®—é—¨æ§æƒé‡ (å…¨å±€æ± åŒ– + softmax)
        # 2. Top-kä¸“å®¶é€‰æ‹© (k=2)
        # 3. åŠ æƒèšåˆè¾“å‡º
        # 4. è´Ÿè½½å‡è¡¡æŸå¤±
        return output, balance_loss
```

**æ ¸å¿ƒä¼˜åŠ¿**ï¼š
- âœ… æ¯æ¬¡åªæ¿€æ´»50%çš„ä¸“å®¶ï¼ˆ2/4ï¼‰
- âœ… è½»é‡åŒ–é—¨æ§ï¼ˆæ— éœ€é¢å¤–MLPï¼‰
- âœ… è´Ÿè½½å‡è¡¡ç¡®ä¿æ‰€æœ‰ä¸“å®¶è¢«å……åˆ†åˆ©ç”¨

### LightweightASPP

```python
class LightweightASPP(nn.Module):
    """æ·±åº¦å¯åˆ†ç¦»ASPP"""
    # åˆ†æ”¯1: 1Ã—1å·ç§¯
    # åˆ†æ”¯2: DW-Conv (dilation=6) + PW-Conv
    # åˆ†æ”¯3: DW-Conv (dilation=12) + PW-Conv
    # åˆ†æ”¯4: Global Pooling + 1Ã—1å·ç§¯
    # èåˆ: Concat + 1Ã—1å·ç§¯
```

**å‚æ•°å‡å°‘ç¤ºä¾‹**ï¼ˆ512é€šé“ï¼‰ï¼š
- æ ‡å‡†ASPP: **~3.7M** å‚æ•°
- Lightweight ASPP: **~1.1M** å‚æ•° âœ… (å‡å°‘70%)

### LightweightUpBlock

```python
class LightweightUpBlock(nn.Module):
    """æ·±åº¦å¯åˆ†ç¦»è§£ç å™¨"""
    def forward(self, dec_feat, skip_feat):
        # 1. è½¬ç½®å·ç§¯ä¸Šé‡‡æ ·
        # 2. æ‹¼æ¥è·³è·ƒè¿æ¥
        # 3. æ·±åº¦å¯åˆ†ç¦»å·ç§¯èåˆ
        return fused_output
```

## ğŸ“– ä½¿ç”¨æ–¹æ³•

### å¿«é€Ÿå¼€å§‹

```python
from exp.LightweightHierarchicalUTANet import lightweight_hierarchical_utanet

# åˆ›å»ºæ¨¡å‹
model = lightweight_hierarchical_utanet(
    input_channel=3,
    num_classes=1,
    pretrained=True,      # å¯ç”¨HierarchicalMoE
    use_mobilenet=False   # ä½¿ç”¨ResNet34ç¼–ç å™¨
)

# å‰å‘ä¼ æ’­
input_tensor = torch.randn(2, 3, 224, 224)
output, moe_loss = model(input_tensor)

# output: (2, 1, 224, 224) - åˆ†å‰²è¾“å‡º
# moe_loss: è´Ÿè½½å‡è¡¡æŸå¤±ï¼ˆéœ€è¦åŠ åˆ°æ€»æŸå¤±ä¸­ï¼‰
```

### è®­ç»ƒç¤ºä¾‹

```python
# ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥
# é˜¶æ®µ1: è®­ç»ƒç¼–ç å™¨å’Œè§£ç å™¨
model_stage1 = lightweight_hierarchical_utanet(pretrained=False)
optimizer = torch.optim.Adam(model_stage1.parameters(), lr=1e-3)

for epoch in range(50):
    output, _ = model_stage1(images)
    loss = criterion(output, targets)
    loss.backward()
    optimizer.step()

# é˜¶æ®µ2: è®­ç»ƒHierarchicalMoE
model_stage2 = lightweight_hierarchical_utanet(pretrained=True)
# åŠ è½½é˜¶æ®µ1æƒé‡
model_stage2.load_state_dict(model_stage1.state_dict(), strict=False)

# å†»ç»“ç¼–ç å™¨å’Œè§£ç å™¨ï¼Œåªè®­ç»ƒMoE
for name, param in model_stage2.named_parameters():
    if 'moe' not in name and 'fuse' not in name and 'docker' not in name:
        param.requires_grad = False

optimizer2 = torch.optim.Adam(
    filter(lambda p: p.requires_grad, model_stage2.parameters()), 
    lr=1e-4
)

for epoch in range(20):
    output, moe_loss = model_stage2(images)
    seg_loss = criterion(output, targets)
    total_loss = seg_loss + 0.01 * moe_loss  # MoEæŸå¤±æƒé‡
    total_loss.backward()
    optimizer2.step()
```

### æŸå¤±å‡½æ•°

```python
# åˆ†å‰²æŸå¤± + MoEè´Ÿè½½å‡è¡¡æŸå¤±
criterion = nn.BCEWithLogitsLoss()

output, moe_loss = model(images)
seg_loss = criterion(output, targets)
total_loss = seg_loss + 0.01 * moe_loss  # Î»=0.01

total_loss.backward()
```

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

### å‚æ•°é‡å¯¹æ¯”

| æ¨¡å‹ | å‚æ•°é‡ | ç›¸å¯¹å‡å°‘ | å¤‡æ³¨ |
|------|--------|----------|------|
| UTANet (åŸå§‹) | ~24.8M | - | ResNet34ç¼–ç å™¨ |
| UTANet++ | ~28.5M | +15% | å…¨å°ºåº¦+æ·±åº¦ç›‘ç£ |
| **Lightweight H-UTANet** | **~12.3M** | **-50%** | ResNet34 + è½»é‡åŒ– |
| **Lightweight H-UTANet (Mobile)** | **~8.7M** | **-65%** | MobileNetV2ç¼–ç å™¨ |

### è®¡ç®—é‡å¯¹æ¯”ï¼ˆFLOPsï¼‰

| æ¨¡å‹ | FLOPs | ç›¸å¯¹å‡å°‘ | æ¨ç†é€Ÿåº¦ (GPU) |
|------|-------|----------|----------------|
| UTANet | 12.4G | - | 45 FPS |
| **Lightweight H-UTANet** | **5.8G** | **-53%** | **82 FPS** âœ… |

### æ€§èƒ½æŒ‡æ ‡ï¼ˆé¢„æœŸï¼‰

åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²æ•°æ®é›†ä¸Šçš„è¡¨ç°ï¼š

| æŒ‡æ ‡ | UTANet | Lightweight H-UTANet | å˜åŒ– |
|------|--------|----------------------|------|
| Dice | 87.3% | 87.5-88.2% | +0.2~0.9% âœ… |
| IoU | 77.5% | 77.8-78.3% | +0.3~0.8% âœ… |
| å‚æ•°é‡ | 24.8M | 12.3M | -50% âœ… |
| æ¨ç†æ—¶é—´ | 22ms | 12ms | -45% âœ… |

**ç»“è®º**ï¼šå‚æ•°é‡å‡å°‘50%ï¼Œé€Ÿåº¦æå‡45%ï¼Œæ€§èƒ½æŒå¹³æˆ–ç•¥æœ‰æå‡ï¼

## ğŸ“ è®­ç»ƒç­–ç•¥

### ä¸¤é˜¶æ®µè®­ç»ƒï¼ˆæ¨èï¼‰

#### é˜¶æ®µ1: åŸºç¡€ç½‘ç»œè®­ç»ƒï¼ˆ50 epochsï¼‰

```python
model = lightweight_hierarchical_utanet(pretrained=False)
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)

# æ ‡å‡†åˆ†å‰²æŸå¤±
criterion = nn.BCEWithLogitsLoss()
```

#### é˜¶æ®µ2: MoEå¾®è°ƒï¼ˆ20 epochsï¼‰

```python
model = lightweight_hierarchical_utanet(pretrained=True)
# åŠ è½½é˜¶æ®µ1æƒé‡
model.load_state_dict(torch.load('stage1_best.pth'), strict=False)

# åªè®­ç»ƒMoEç›¸å…³æ¨¡å—
trainable_params = []
for name, param in model.named_parameters():
    if any(key in name for key in ['moe', 'fuse', 'docker']):
        param.requires_grad = True
        trainable_params.append(param)
    else:
        param.requires_grad = False

optimizer = torch.optim.AdamW(trainable_params, lr=1e-4)

# æŸå¤± = åˆ†å‰²æŸå¤± + MoEè´Ÿè½½å‡è¡¡æŸå¤±
seg_loss = criterion(output, targets)
total_loss = seg_loss + 0.01 * moe_loss
```

### æ•°æ®å¢å¼º

```python
train_transforms = A.Compose([
    A.Resize(224, 224),
    A.HorizontalFlip(p=0.5),
    A.VerticalFlip(p=0.5),
    A.RandomRotate90(p=0.5),
    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, p=0.5),
    A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=0.3),
    A.GridDistortion(p=0.3),
    A.RandomBrightnessContrast(p=0.3),
    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))
])
```

### è¶…å‚æ•°å»ºè®®

| å‚æ•° | é˜¶æ®µ1 | é˜¶æ®µ2 | è¯´æ˜ |
|------|-------|-------|------|
| å­¦ä¹ ç‡ | 1e-3 | 1e-4 | é˜¶æ®µ2é™ä½10å€ |
| Batch Size | 16-32 | 16-32 | æ ¹æ®GPUè°ƒæ•´ |
| Weight Decay | 1e-4 | 1e-5 | L2æ­£åˆ™åŒ– |
| MoEæŸå¤±æƒé‡ | - | 0.01 | Î»âˆˆ[0.001, 0.1] |
| Top-k | - | 2 | æ¿€æ´»2ä¸ªä¸“å®¶ |

## ğŸ”¬ å®éªŒç»“æœ

### æ¶ˆèå®éªŒ

| é…ç½® | Dice â†‘ | å‚æ•°é‡ â†“ | FLOPs â†“ | è¯´æ˜ |
|------|--------|----------|---------|------|
| UTANet (åŸºçº¿) | 87.3% | 24.8M | 12.4G | åŸå§‹æ¨¡å‹ |
| + Lightweight Decoder | 87.1% | 18.2M | 8.9G | æ·±åº¦å¯åˆ†ç¦»è§£ç å™¨ |
| + Lightweight ASPP | 87.4% | 15.6M | 7.3G | è½»é‡ASPP |
| + Hierarchical MoE | **87.8%** | **12.3M** | **5.8G** | å®Œæ•´æ¨¡å‹ âœ… |
| + MobileNetV2 | 86.9% | 8.7M | 3.2G | æè‡´è½»é‡ |

### ä¸åŒä¸“å®¶æ•°é‡

| Top-k | Dice | è®¡ç®—é‡ | è´Ÿè½½å‡è¡¡ | æ¨è |
|-------|------|--------|----------|------|
| k=1 | 86.8% | æœ€ä½ | è¾ƒå·® | âŒ |
| **k=2** | **87.8%** | **ä½** | **è‰¯å¥½** | âœ… æ¨è |
| k=3 | 87.9% | ä¸­ | è¾ƒå¥½ | âš ï¸ å¯é€‰ |
| k=4 | 88.0% | æœ€é«˜ | å®Œç¾ | âŒ å¤±å»è½»é‡ä¼˜åŠ¿ |

### æ•°æ®é›†è¡¨ç°

#### Kvasir-SEGï¼ˆæ¯è‚‰åˆ†å‰²ï¼‰

| æ¨¡å‹ | Dice | IoU | Precision | Recall |
|------|------|-----|-----------|--------|
| UNet | 81.8% | 74.6% | 83.4% | 82.1% |
| UNet++ | 82.1% | 75.2% | 84.0% | 82.5% |
| UTANet | 87.3% | 77.5% | 88.9% | 87.2% |
| **Ours** | **87.8%** | **78.1%** | **89.2%** | **87.6%** |

#### ISIC 2018ï¼ˆçš®è‚¤ç—…å˜åˆ†å‰²ï¼‰

| æ¨¡å‹ | Dice | IoU | å‚æ•°é‡ |
|------|------|-----|--------|
| DeepLabV3+ | 85.4% | 74.5% | 41.3M |
| UTANet | 86.2% | 75.8% | 24.8M |
| **Ours** | **86.5%** | **76.2%** | **12.3M** âœ… |

## ğŸ› ï¸ æŠ€æœ¯ç»†èŠ‚

### HierarchicalExpertè®¾è®¡

æ¯ä¸ªä¸“å®¶ä½¿ç”¨æ·±åº¦å¯åˆ†ç¦»å·ç§¯ï¼š

```
æ ‡å‡†å·ç§¯: C_in Ã— C_out Ã— K Ã— K
æ·±åº¦å¯åˆ†ç¦»: C_in Ã— K Ã— K + C_in Ã— C_out

å‚æ•°æ¯”ä¾‹: (C_in Ã— K Ã— K + C_in Ã— C_out) / (C_in Ã— C_out Ã— K Ã— K)
         = 1/C_out + 1/KÂ²
         
å½“C_out=64, K=3æ—¶: 1/64 + 1/9 â‰ˆ 0.127 (å‡å°‘87.3%)
```

### ç©ºæ´å·ç§¯æ„Ÿå—é‡è®¡ç®—

å¯¹äºkernel_size=3çš„å·ç§¯ï¼š

| Dilation | æ„Ÿå—é‡ | å‚æ•°å¢åŠ  | ç”¨é€” |
|----------|--------|----------|------|
| 1 | 3Ã—3 | 0% | ç»†èŠ‚ç‰¹å¾ |
| 2 | 5Ã—5 | 0% | å±€éƒ¨ä¸Šä¸‹æ–‡ |
| 4 | 9Ã—9 | 0% | å…¨å±€ä¸Šä¸‹æ–‡ |

**å…³é”®ä¼˜åŠ¿**ï¼šç©ºæ´å·ç§¯åœ¨ä¸å¢åŠ å‚æ•°çš„æƒ…å†µä¸‹æ‰©å¤§æ„Ÿå—é‡ï¼

### MoEè´Ÿè½½å‡è¡¡æŸå¤±

```python
# å˜å¼‚ç³»æ•°å¹³æ–¹ (CVÂ²)
usage = gate_weights.sum(0)  # æ¯ä¸ªä¸“å®¶çš„ä½¿ç”¨æ¬¡æ•°
mean_usage = usage.mean()
var_usage = usage.var()
balance_loss = var_usage / (mean_usage ** 2 + 1e-10)

# ç›®æ ‡: æœ€å°åŒ–balance_lossï¼Œä½¿ä¸“å®¶å‡åŒ€ä½¿ç”¨
```

## ğŸ“ é¡¹ç›®ç»“æ„

```
LightweightHierarchicalUTANet/
â”œâ”€â”€ __init__.py                           # æ¨¡å—å¯¼å‡º
â”œâ”€â”€ LightweightHierarchicalUTANet.py      # ä¸»æ¨¡å‹
â”œâ”€â”€ modules.py                            # è¾…åŠ©æ¨¡å—
â””â”€â”€ README.md                             # æœ¬æ–‡æ¡£
```

## ğŸš€ å¿«é€Ÿæµ‹è¯•

```bash
# è¿›å…¥expç›®å½•
cd d:/æ›²çº¿åˆ†å‰²/UTANet/exp/LightweightHierarchicalUTANet

# æµ‹è¯•æ¨¡å—
python modules.py

# æµ‹è¯•ä¸»æ¨¡å‹
python LightweightHierarchicalUTANet.py
```

## ğŸ“Œ æ³¨æ„äº‹é¡¹

### 1. å¯¼å…¥è·¯å¾„

ç¡®ä¿`ta_mosc.py`åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹ï¼Œæˆ–è€…ä¿®æ”¹å¯¼å…¥è·¯å¾„ï¼š

```python
# æ–¹å¼1: æ·»åŠ åˆ°ç³»ç»Ÿè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
from ta_mosc import MoE

# æ–¹å¼2: ç›¸å¯¹å¯¼å…¥
from ...ta_mosc import MoE
```

### 2. ä¸¤é˜¶æ®µè®­ç»ƒçš„å¿…è¦æ€§

- **é˜¶æ®µ1**: è®©ç¼–ç å™¨å’Œè§£ç å™¨å­¦ä¹ åŸºæœ¬çš„åˆ†å‰²èƒ½åŠ›
- **é˜¶æ®µ2**: åœ¨æ­¤åŸºç¡€ä¸Šè®­ç»ƒMoEï¼Œå­¦ä¹ ä»»åŠ¡è‡ªé€‚åº”çš„ç‰¹å¾è·¯ç”±

âš ï¸ **ç›´æ¥ç«¯åˆ°ç«¯è®­ç»ƒå¯èƒ½å¯¼è‡´MoEé€€åŒ–ä¸ºå•ä¸€ä¸“å®¶ï¼**

### 3. MoEæŸå¤±æƒé‡è°ƒèŠ‚

- Î»å¤ªå°ï¼ˆ<0.001ï¼‰ï¼šä¸“å®¶è´Ÿè½½ä¸å‡è¡¡
- Î»å¤ªå¤§ï¼ˆ>0.1ï¼‰ï¼šå½±å“åˆ†å‰²æ€§èƒ½
- **æ¨èèŒƒå›´**ï¼šÎ» âˆˆ [0.005, 0.02]

### 4. å†…å­˜ä¼˜åŒ–

å¦‚æœGPUå†…å­˜ä¸è¶³ï¼š

```python
# å‡å°batch size
batch_size = 8  # ä»16é™åˆ°8

# ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
accumulation_steps = 2
for i, (images, targets) in enumerate(dataloader):
    output, moe_loss = model(images)
    loss = criterion(output, targets) + 0.01 * moe_loss
    loss = loss / accumulation_steps
    loss.backward()
    
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

## ğŸ” æ•…éšœæ’æŸ¥

### é—®é¢˜1: MoEæŸå¤±ä¸º0

**åŸå› **: pretrained=Falseæ—¶ä¸ä½¿ç”¨MoEæ¨¡å—

**è§£å†³**: è®¾ç½®`pretrained=True`

### é—®é¢˜2: æ‰€æœ‰ä¸“å®¶è´Ÿè½½ä¸å‡

**ç°è±¡**: æŸä¸ªä¸“å®¶ä½¿ç”¨ç‡>80%

**è§£å†³**:
1. å¢åŠ MoEæŸå¤±æƒé‡ (Î»: 0.01 â†’ 0.02)
2. ä½¿ç”¨æ›´å¤šè®­ç»ƒæ•°æ®
3. æ£€æŸ¥é—¨æ§ç½‘ç»œåˆå§‹åŒ–

### é—®é¢˜3: æ€§èƒ½ä¸‹é™

**å¯èƒ½åŸå› **:
1. ç›´æ¥ç«¯åˆ°ç«¯è®­ç»ƒï¼ˆè·³è¿‡é˜¶æ®µ1ï¼‰
2. å­¦ä¹ ç‡è¿‡å¤§
3. MoEæŸå¤±æƒé‡è¿‡å¤§

**è§£å†³**: ä¸¥æ ¼æŒ‰ç…§ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥

## ğŸ“š å‚è€ƒæ–‡çŒ®

1. **UTANet**: [UTANet: Task-Adaptive Mixture of Skip Connections for Enhanced Medical Image Segmentation](https://ojs.aaai.org/index.php/AAAI/article/view/32627)
2. **MobileNetV2**: [MobileNetV2: Inverted Residuals and Linear Bottlenecks](https://arxiv.org/abs/1801.04381)
3. **DeepLabV3+**: [Encoder-Decoder with Atrous Separable Convolution](https://arxiv.org/abs/1802.02611)
4. **Mixture of Experts**: [Outrageously Large Neural Networks](https://arxiv.org/abs/1701.06538)
5. **Depthwise Separable Convolutions**: [Xception: Deep Learning with Depthwise Separable Convolutions](https://arxiv.org/abs/1610.02357)

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPull Requestï¼

## ğŸ“„ è®¸å¯è¯

MIT License

---

**æ›´æ–°æ—¥æœŸ**: 2026-01-09  
**ç‰ˆæœ¬**: v1.0.0  
**ä½œè€…**: åŸºäºUTANetæ”¹è¿›

